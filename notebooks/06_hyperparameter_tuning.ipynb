{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T20:02:05.811979Z",
     "start_time": "2025-09-03T20:01:52.898259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Heart Disease UCI Dataset - Hyperparameter Tuning\n",
    "# Notebook 06: Model Optimization & Final Model Selection\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, make_scorer\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load selected features data\n",
    "df = pd.read_csv('../data/processed/selected_features.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Target value counts:\")\n",
    "print(y.value_counts().sort_index())\n",
    "print(f\"Number of unique target values: {y.nunique()}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale features for algorithms that need scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Baseline results for comparison\n",
    "try:\n",
    "    baseline_results = pd.read_csv('../models/model_results.csv', index_col=0)\n",
    "    print(\"\\nBaseline Results:\")\n",
    "    print(baseline_results[['Accuracy', 'F1-Score']].round(3))\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nBaseline results file not found. Will create comparison after tuning.\")\n",
    "    baseline_results = None\n",
    "\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'C': [0.1, 1, 10, 100], # Lower = more regularization, higher = less regularization\n",
    "            'solver': ['liblinear', 'lbfgs'],\n",
    "            'max_iter': [1000]\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'max_depth': [3, 5, 10, 15, None], # Controls overfitting (None = unlimited depth)\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100, 200], #  # Number of trees\n",
    "            'max_depth': [3, 5, 10, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Create F1 scorer for binary classification\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Hyperparameter tuning results\n",
    "tuning_results = {}\n",
    "best_models = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "\n",
    "    # Determine if we need scaled data\n",
    "    use_scaled_data = name in ['Logistic Regression', 'SVM']\n",
    "    X_train_use = X_train_scaled if use_scaled_data else X_train\n",
    "    X_test_use = X_test_scaled if use_scaled_data else X_test\n",
    "\n",
    "    try:\n",
    "        # Use GridSearchCV with F1 scorer\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grids[name],\n",
    "            cv=5,\n",
    "            scoring=f1_scorer,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train_use, y_train)\n",
    "\n",
    "        # Best parameters\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best CV F1-score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "        # Test on test set\n",
    "        y_pred = grid_search.predict(X_test_use)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "        print(f\"Test F1-score: {test_f1:.3f}\")\n",
    "\n",
    "        # Results\n",
    "        tuning_results[name] = {\n",
    "            'Best_Params': grid_search.best_params_,\n",
    "            'CV_F1': grid_search.best_score_,\n",
    "            'Test_Accuracy': test_accuracy,\n",
    "            'Test_F1': test_f1,\n",
    "            'Uses_Scaled_Data': use_scaled_data\n",
    "        }\n",
    "\n",
    "        best_models[name] = {\n",
    "            'model': grid_search.best_estimator_,\n",
    "            'scaler': scaler if use_scaled_data else None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error tuning {name}: {str(e)}\")\n",
    "        print(\"Skipping this model...\")\n",
    "        continue\n",
    "\n",
    "# Create comparison DataFrame\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"OPTIMIZED MODEL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if tuning_results:\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': tuning_results.keys(),\n",
    "        'CV_F1': [tuning_results[name]['CV_F1'] for name in tuning_results.keys()],\n",
    "        'Test_Accuracy': [tuning_results[name]['Test_Accuracy'] for name in tuning_results.keys()],\n",
    "        'Test_F1': [tuning_results[name]['Test_F1'] for name in tuning_results.keys()]\n",
    "    }).set_index('Model')\n",
    "\n",
    "    print(results_df.round(3))\n",
    "\n",
    "    # Compare with baseline if available\n",
    "    if baseline_results is not None:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"BASELINE vs OPTIMIZED COMPARISON\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Create comparison for models that exist in both\n",
    "        common_models = set(baseline_results.index) & set(tuning_results.keys())\n",
    "\n",
    "        if common_models:\n",
    "            comparison_data = {}\n",
    "            for model_name in common_models:\n",
    "                comparison_data[model_name] = {\n",
    "                    'Baseline_Accuracy': baseline_results.loc[model_name, 'Accuracy'],\n",
    "                    'Optimized_Accuracy': tuning_results[model_name]['Test_Accuracy'],\n",
    "                    'Baseline_F1': baseline_results.loc[model_name, 'F1-Score'],\n",
    "                    'Optimized_F1': tuning_results[model_name]['Test_F1'],\n",
    "                }\n",
    "\n",
    "            comparison_df = pd.DataFrame(comparison_data).T\n",
    "            comparison_df['Accuracy_Improvement'] = comparison_df['Optimized_Accuracy'] - comparison_df[\n",
    "                'Baseline_Accuracy']\n",
    "            comparison_df['F1_Improvement'] = comparison_df['Optimized_F1'] - comparison_df['Baseline_F1']\n",
    "\n",
    "            print(comparison_df.round(3))\n",
    "        else:\n",
    "            print(\"No common models found between baseline and optimized results.\")\n",
    "\n",
    "    # Find best optimized model\n",
    "    # Selects model with the highest test F1-score (most reliable)\n",
    "    best_model_name = max(tuning_results.keys(), key=lambda x: tuning_results[x]['Test_F1'])\n",
    "    best_optimized_model_info = best_models[best_model_name]\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(\"BEST OPTIMIZED MODEL\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Model: {best_model_name}\")\n",
    "    print(f\"Parameters: {tuning_results[best_model_name]['Best_Params']}\")\n",
    "    print(f\"CV F1-score: {tuning_results[best_model_name]['CV_F1']:.3f}\")\n",
    "    print(f\"Test F1-score: {tuning_results[best_model_name]['Test_F1']:.3f}\")\n",
    "    print(f\"Test Accuracy: {tuning_results[best_model_name]['Test_Accuracy']:.3f}\")\n",
    "\n",
    "    model_for_prediction = best_optimized_model_info['model']\n",
    "    scaler_for_prediction = best_optimized_model_info['scaler']\n",
    "\n",
    "    X_test_final = X_test_scaled if scaler_for_prediction else X_test\n",
    "    y_pred_best = model_for_prediction.predict(X_test_final)\n",
    "\n",
    "    print(f\"\\nClassification Report for {best_model_name}:\")\n",
    "    print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "    # Feature importance for tree-based models\n",
    "    if best_model_name in ['Decision Tree', 'Random Forest']:\n",
    "        try:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': model_for_prediction.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            print(f\"\\nTop 10 Feature Importances ({best_model_name}):\")\n",
    "            print(feature_importance.head(10).round(4))\n",
    "        except:\n",
    "            print(\"Could not extract feature importances.\")\n",
    "\n",
    "    # Save the best optimized model\n",
    "    best_model_package = {\n",
    "        'model': model_for_prediction,\n",
    "        'scaler': scaler_for_prediction,\n",
    "        'feature_names': X.columns.tolist(),\n",
    "        'model_name': best_model_name,\n",
    "        'parameters': tuning_results[best_model_name]['Best_Params'],\n",
    "        'performance': {\n",
    "            'cv_f1': tuning_results[best_model_name]['CV_F1'],\n",
    "            'test_f1': tuning_results[best_model_name]['Test_F1'],\n",
    "            'test_accuracy': tuning_results[best_model_name]['Test_Accuracy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    joblib.dump(best_model_package, '../models/best_optimized_model.pkl')\n",
    "    print(f\"\\nBest optimized model package saved to ../models/best_optimized_model.pkl\")\n",
    "\n",
    "    # Save tuning results\n",
    "    tuning_results_df = pd.DataFrame({\n",
    "        k: {\n",
    "            'CV_F1': v['CV_F1'],\n",
    "            'Test_Accuracy': v['Test_Accuracy'],\n",
    "            'Test_F1': v['Test_F1'],\n",
    "            'Best_Params': str(v['Best_Params'])\n",
    "        }\n",
    "        for k, v in tuning_results.items()\n",
    "    }).T\n",
    "\n",
    "    tuning_results_df.to_csv('../models/tuning_results.csv')\n",
    "    print(\"Tuning results saved to ../models/tuning_results.csv\")\n",
    "\n",
    "    # Save comparison results if baseline exists\n",
    "    if baseline_results is not None and 'comparison_df' in locals():\n",
    "        comparison_df.to_csv('../models/model_comparison.csv')\n",
    "        print(\"Model comparison saved to ../models/model_comparison.csv\")\n",
    "\n",
    "    print(\"\\nHyperparameter tuning completed successfully!\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Number of models tuned: {len(tuning_results)}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Best F1-score: {tuning_results[best_model_name]['Test_F1']:.3f}\")\n",
    "    if baseline_results is not None and best_model_name in baseline_results.index:\n",
    "        improvement = tuning_results[best_model_name]['Test_F1'] - baseline_results.loc[best_model_name, 'F1-Score']\n",
    "        print(f\"F1-score improvement: {improvement:+.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No models were successfully tuned.\")"
   ],
   "id": "42279246b84be93e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (297, 8)\n",
      "Target value counts:\n",
      "target\n",
      "0    160\n",
      "1    137\n",
      "Name: count, dtype: int64\n",
      "Number of unique target values: 2\n",
      "Classification type: Binary\n",
      "F1-score averaging method: binary\n",
      "\n",
      "Training set: (237, 8)\n",
      "Test set: (60, 8)\n",
      "\n",
      "Baseline Results:\n",
      "                     Accuracy  F1-Score\n",
      "Logistic Regression     0.850     0.830\n",
      "Decision Tree           0.733     0.704\n",
      "Random Forest           0.817     0.784\n",
      "SVM                     0.833     0.808\n",
      "\n",
      "==================================================\n",
      "HYPERPARAMETER TUNING\n",
      "==================================================\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Best parameters: {'C': 0.1, 'max_iter': 1000, 'solver': 'liblinear'}\n",
      "Best CV F1-score: 0.802\n",
      "Test Accuracy: 0.867\n",
      "Test F1-score: 0.852\n",
      "\n",
      "Tuning Decision Tree...\n",
      "Best parameters: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best CV F1-score: 0.781\n",
      "Test Accuracy: 0.817\n",
      "Test F1-score: 0.800\n",
      "\n",
      "Tuning Random Forest...\n",
      "Best parameters: {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best CV F1-score: 0.797\n",
      "Test Accuracy: 0.833\n",
      "Test F1-score: 0.808\n",
      "\n",
      "Tuning SVM...\n",
      "Best parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best CV F1-score: 0.791\n",
      "Test Accuracy: 0.850\n",
      "Test F1-score: 0.830\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED MODEL RESULTS\n",
      "==================================================\n",
      "                     CV_F1  Test_Accuracy  Test_F1\n",
      "Model                                             \n",
      "Logistic Regression  0.802          0.867    0.852\n",
      "Decision Tree        0.781          0.817    0.800\n",
      "Random Forest        0.797          0.833    0.808\n",
      "SVM                  0.791          0.850    0.830\n",
      "\n",
      "==================================================\n",
      "BASELINE vs OPTIMIZED COMPARISON\n",
      "==================================================\n",
      "                     Baseline_Accuracy  Optimized_Accuracy  Baseline_F1  \\\n",
      "Logistic Regression              0.850               0.867        0.830   \n",
      "Decision Tree                    0.733               0.817        0.704   \n",
      "Random Forest                    0.817               0.833        0.784   \n",
      "SVM                              0.833               0.850        0.808   \n",
      "\n",
      "                     Optimized_F1  Accuracy_Improvement  F1_Improvement  \n",
      "Logistic Regression         0.852                 0.017           0.022  \n",
      "Decision Tree               0.800                 0.083           0.096  \n",
      "Random Forest               0.808                 0.017           0.023  \n",
      "SVM                         0.830                 0.017           0.022  \n",
      "\n",
      "========================================\n",
      "BEST OPTIMIZED MODEL\n",
      "========================================\n",
      "Model: Logistic Regression\n",
      "Parameters: {'C': 0.1, 'max_iter': 1000, 'solver': 'liblinear'}\n",
      "CV F1-score: 0.802\n",
      "Test F1-score: 0.852\n",
      "Test Accuracy: 0.867\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88        32\n",
      "           1       0.88      0.82      0.85        28\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.87      0.86      0.87        60\n",
      "weighted avg       0.87      0.87      0.87        60\n",
      "\n",
      "\n",
      "Best optimized model package saved to ../models/best_optimized_model.pkl\n",
      "Tuning results saved to ../models/tuning_results.csv\n",
      "Model comparison saved to ../models/model_comparison.csv\n",
      "\n",
      "Hyperparameter tuning completed successfully!\n",
      "\n",
      "========================================\n",
      "SUMMARY\n",
      "========================================\n",
      "Classification Type: Binary\n",
      "Number of models tuned: 4\n",
      "Best model: Logistic Regression\n",
      "Best F1-score: 0.852\n",
      "F1-score improvement: +0.022\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
